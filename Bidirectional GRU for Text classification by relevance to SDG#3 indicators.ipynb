{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c2f549b769bfc4e75d9bda6f234b00aef0c64cc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cc65d79cac4b095fa303fb56c43663418124b10",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/crawl300d2m/crawl-300d-2M.vec'\n",
    "\n",
    "train=pd.read_csv('../input/sdg-train-test-dataset/Devex_train.csv',encoding='latin-1')\n",
    "test=pd.read_csv('../input/sdg-train-test-dataset/Devex_test_questions.csv',encoding='latin-1')\n",
    "submission=pd.read_csv('../input/sdg-train-test-dataset/Devex_submission_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32e68d8f57dbab29298d580b3a767b03e5fb6662",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Text'] = train['Text'].apply(lambda x: bs4.BeautifulSoup(x, 'lxml').get_text())\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb19da65bbe688ea4cdc4768368ae6a8100166a6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names =list(submission.columns.values)[1:]\n",
    "train = train.reindex(columns=train.columns.tolist() + class_names)\n",
    "#filling the NaN values\n",
    "train=train.select_dtypes(include=['object','float']).fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7945819ca1a84847b9ee705d5b2879e7dfd7ff9b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming the target variables into a useful format\n",
    "train[\"targets\"]=train[\"Label 1\"].apply(lambda x: x[:5] )\n",
    "targets=pd.get_dummies(train[\"targets\"])\n",
    "#targets.select_dtypes(include=['object']).fillna(0)\n",
    "for column in train.columns[4:15]:\n",
    "    train[\"targets\"]=train[column].apply(lambda x: x[:5] if (type(x)==str) else \"None\")\n",
    "    new_targets=pd.get_dummies(train[\"targets\"])\n",
    "    diff=list(set(list(targets.columns))-set(list(new_targets.columns)))\n",
    "    new_targets[diff]=targets[diff]*0\n",
    "    #print(len(new_targets.columns))\n",
    "    targets=targets + new_targets\n",
    "    #print(targets[\"3.b.2\"][0])\n",
    "    if column==\"Label 12\":\n",
    "        break\n",
    "#targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "861fc6033d5a57d3f74f81530049918703bd83e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = train['Text']\n",
    "test_text = test['Text']\n",
    "\n",
    "X_train = train_text.fillna(\"fillna\").values\n",
    "y_train = targets[class_names].values\n",
    "X_test = test_text.fillna(\"fillna\").values\n",
    "\n",
    "\n",
    "\n",
    "max_features = 30000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "# converting the training text into intergers for training\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# importing the trained embeddings\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c4a2a9aeb531001da10f22fe4417588dee72796",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the deep neural network model(Bidirectional)\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(27, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "#Balancing the target classes\n",
    "sample_weights = class_weight.compute_sample_weight('balanced', y_tra)\n",
    "#Model training\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),sample_weight=sample_weights,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff021327277f9e29b1f0b3c721f390f49dbdc4f1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model prediction \n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[class_names] = y_pred\n",
    "submission.to_csv('submission_gru.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62a67e3f83267049684a5e2a9ca2109296865360",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submitted=pd.read_csv('submission_gru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d77bcee66a1af14563dd2bfd47740e425efd0c36",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting model output to 1's and 0's for submission\n",
    "for column in submitted.columns[1:]:\n",
    "    submitted[column]=submitted[column].apply(lambda x: 1 if (x >=.98 ) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "668c1cca3f68f572999a3a4c51efbe713caf5310",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submitted.to_csv('./submission_gru_98.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23338378387330b7f5214bdc07a40be78e55060d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the trained model.\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
